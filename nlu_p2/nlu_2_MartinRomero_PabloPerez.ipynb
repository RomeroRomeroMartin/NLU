{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Needed imports and versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "stl9coFCKbuJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import requests\n",
    "import os\n",
    "import copy\n",
    "from preprocess import *\n",
    "from oracle import *\n",
    "from prediction import *\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PdRMMf4dKhus",
    "outputId": "ee06882a-5fb8-4661-f662-590a799b0954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iz5Igp2rLguP",
    "outputId": "8cb0ac56-3c76-4ce3-e8d8-fa19433f164f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.1\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and preprocessing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gQm7-UwzLiuR"
   },
   "outputs": [],
   "source": [
    "def get_dataset(url, name):\n",
    "    filename = os.path.join(os.getcwd(), name) #we save on the working directory\n",
    "\n",
    "    r = requests.get(url) #get the webpage\n",
    "    with open(filename, 'w', encoding=\"utf-8\") as f: #and write it to a file\n",
    "      f.write(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QtYZgrLfLmNZ"
   },
   "outputs": [],
   "source": [
    "# download the datasets from github\n",
    "get_dataset(\"https://raw.githubusercontent.com/UniversalDependencies/UD_English-ParTUT/master/en_partut-ud-train.conllu\", \"./datasets/original/en_partut-ud-train.conllu\")\n",
    "get_dataset(\"https://raw.githubusercontent.com/UniversalDependencies/UD_English-ParTUT/master/en_partut-ud-dev.conllu\", \"./datasets/original/en_partut-ud-dev.conllu\")\n",
    "get_dataset(\"https://raw.githubusercontent.com/UniversalDependencies/UD_English-ParTUT/master/en_partut-ud-test.conllu\", \"./datasets/original/en_partut-ud-test.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "aWjOnQ_oL5eB"
   },
   "outputs": [],
   "source": [
    "# preprocess the datasets to remove commentaries, multiwords and empty tokens\n",
    "preprocess_dataset(\"./datasets/original/en_partut-ud-train.conllu\", \"./datasets/original/my_en_train.conllu\")\n",
    "preprocess_dataset(\"./datasets/original/en_partut-ud-dev.conllu\", \"./datasets/original/my_en_dev.conllu\")\n",
    "preprocess_dataset(\"./datasets/original/en_partut-ud-test.conllu\", \"./datasets/original/my_en_test.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DEnTgmb8TqkW"
   },
   "outputs": [],
   "source": [
    "# generate a dataframe for every dataset, removing non_projective sentences and adding ROOT\n",
    "df_train = generate_dataframe(\"./datasets/original/my_en_train.conllu\")\n",
    "df_val = generate_dataframe(\"./datasets/original/my_en_dev.conllu\")\n",
    "df_test = generate_dataframe(\"./datasets/original/my_en_test.conllu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the arc-eager oracle to generate training samples for our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "79D1DxvKagpq"
   },
   "outputs": [],
   "source": [
    "# dictionaries to encode the actions, the part-of-speech tags, and the dependency relations\n",
    "actions = {'left-arc': 1, 'right-arc': 2, 'shift': 3, 'reduce': 4, 'end': 5}\n",
    "\n",
    "pos={'ADJ': 1, 'ADP': 2, 'ADV': 3, 'AUX': 4, 'CCONJ': 5, 'DET': 6, 'INTJ': 7, 'NOUN': 8, 'NUM': 9, 'PART': 10, 'PRON': 11, 'PROPN': 12,\n",
    "         'PUNCT': 13, 'SCONJ': 14, 'SYM': 15, 'VERB': 16, 'X': 17}\n",
    "\n",
    "deprel={'root': 1, 'nsubj': 2, 'obj': 3, 'iobj': 4, 'csubj': 5, 'ccomp': 6, 'xcomp': 7, 'obl': 8, 'vocative': 9, 'expl': 10, 'dislocated': 11,\n",
    "        'advcl': 12, 'advmod': 13, 'discourse': 14, 'aux': 15, 'cop': 16, 'mark': 17, 'nmod': 18, 'appos': 19, 'nummod': 20, 'acl': 21,\n",
    "        'amod': 22, 'det': 23, 'clf': 24, 'case': 25, 'conj': 26, 'cc': 27, 'fixed': 28, 'flat': 29, 'list': 30, 'parataxis': 31, 'compound': 32,\n",
    "        'orphan': 33, 'goeswith': 34, 'reparandum': 35, 'punct': 36, 'dep': 37, 'acl:relcl': 38, 'advcl:relcl': 39, 'advmod:emph': 40,\n",
    "        'advmod:lmod': 41, 'aux:pass': 42, 'cc:preconj': 43, 'compound:lvc': 44, 'compound:prt': 45, 'compound:redup': 46, 'compound:svc': 47,\n",
    "        'csubj:outer': 48, 'csubj:pass': 49, 'det:numgov': 50, 'det:nummod': 51, 'det:poss': 52, 'expl:impers': 53, 'expl:pass': 54,\n",
    "        'expl:pv': 55, 'flat:foreign': 56, 'nmod:poss': 57, 'nmod:tmod': 58, 'nsubj:outer': 59, 'nsubj:pass': 60, 'nummod:gov': 61,\n",
    "        'obl:agent': 62, 'obl:arg': 63, 'obl:lmod': 64, 'obl:tmod': 65, 'det:predet': 66, 'nmod:npmod': 67}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "czDLof_SCTk9"
   },
   "outputs": [],
   "source": [
    "# we also need the inverse dictionaries for actions and dependency relations, to \"trace back\" the outputs\n",
    "# of our models\n",
    "inv_actions = {v: k for k, v in actions.items()}\n",
    "\n",
    "inv_deprel = {v: k for k, v in deprel.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "60LreyEEWJpW"
   },
   "outputs": [],
   "source": [
    "# this function will take a dataframe, a text_vectorizer vocabulary, and the number of features to extract from \n",
    "# stack and buffer, and apply our arc-eager oracle to generate an execution trace and process it so it can be \n",
    "# used as the training input of a predictive model\n",
    "def process_traces(dataframe, vocabulario, n_w):\n",
    "  data_list=[] # list to store the training inputs\n",
    "  for i in range(len(dataframe)): # for each element in the dataframe\n",
    "    items = dataframe.iloc[i][1] # we extract the items (words and their features)\n",
    "    arcs = dataframe.iloc[i][2] # we extract the arcs\n",
    "    traza = oracle(items, arcs) # we use the oracle to generate an execution trace\n",
    "\n",
    "    # we generate an array with the words in the original sentence, and then an equivalent one that contains their respective\n",
    "    # \"token\" form, obtained from our vocabulary. The second line is long because it needs to check if the word exists in lowercase,\n",
    "    # in normal case, or if it does not exist at all (token 1, Out Of Vocabulary).\n",
    "    words=[items[k][1] for k in range(len(items))]\n",
    "    vectorizer_index = [vocabulario.index(words[w].lower()) if words[w].lower() in vocabulario else vocabulario.index(words[w]) if words[w] in vocabulario else 1 for w in range(1,len(words))]\n",
    "    # we insert an \"artificial\" token at the beggining, that represents ROOT\n",
    "    vectorizer_index.insert(0, len(vocabulario))\n",
    "\n",
    "    for j in range(len(traza)): #for each step of the trace\n",
    "      stack=traza[j][0] # extract the different features of the state\n",
    "      buffer=traza[j][1]\n",
    "      action=traza[j][2]\n",
    "      deps=traza[j][3]\n",
    "\n",
    "      action = actions[action] # get the integer corresponding to the action\n",
    "      if action < 3: # if its an arc\n",
    "        dependency = deprel[deps[-1][1]] # establish the dependency\n",
    "      else: # if it is not, then we ignore the dependency\n",
    "        dependency = 0\n",
    "      pos_stack=[]\n",
    "      pos_buffer=[]\n",
    "\n",
    "      # this code structure will be repeated, but it basically extracts n_w tokens from the end of the stack and the start of the buffer\n",
    "      # and pads the arrays with 0 if there weren't enough tokens to fill n_w spaces.\n",
    "      stack_sel=list(np.array(np.pad(stack[-n_w:], (n_w-len(stack[-n_w:]), 0), 'constant', constant_values=(0, 0)), dtype=np.int32))\n",
    "      buffer_sel=list(np.array(np.pad(buffer[0:n_w], (0, n_w-len(buffer[0:n_w])), 'constant', constant_values=(0, 0)), dtype=np.int32))\n",
    "      \n",
    "      # for each token extracted from the stack and buffer, we obtain its part of speech from the sentence items. If the token extracted\n",
    "      # was a zero (padding), we insert a 0.\n",
    "      for element in stack_sel:\n",
    "        if element == 0:\n",
    "          pos_stack.append(0)\n",
    "        else:\n",
    "          pos_stack.append(pos[items[element][3]])\n",
    "      for element in buffer_sel:\n",
    "        if element == 0:\n",
    "          pos_buffer.append(0)\n",
    "        else:\n",
    "          pos_buffer.append(pos[items[element][3]])\n",
    "\n",
    "      # we create an equivalent stack and buffer using the tokenized form of the words instead of the baseline ids (remember that each\n",
    "      # sentence considers itself as using ids 0, 1, 2..., but we need to supply the tokenized words to the model)\n",
    "      new_stack=[vectorizer_index[indice] for indice in stack]\n",
    "      new_buffer=[vectorizer_index[indice] for indice in buffer]\n",
    "\n",
    "      # we extract the tokenized forms of the words as done above\n",
    "      stack_sel=list(np.array(np.pad(new_stack[-n_w:], (n_w-len(new_stack[-n_w:]), 0), 'constant', constant_values=(0, 0)), dtype=np.int32))\n",
    "      buffer_sel=list(np.array(np.pad(new_buffer[0:n_w], (0, n_w-len(new_buffer[0:n_w])), 'constant', constant_values=(0, 0)), dtype=np.int32))\n",
    "      \n",
    "      # we append the features of the input sample to our data list\n",
    "      data_list.append([stack_sel, buffer_sel, pos_stack, pos_buffer, action, dependency])\n",
    "  # we build and return a dataframe with the data list\n",
    "  return pd.DataFrame(data_list, columns=['stack_tokens', 'buffer_tokens', 'stack_pos', 'buffer_pos', 'action', 'dependency'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0A2ke5KhXXn_"
   },
   "outputs": [],
   "source": [
    "# we train a text vectorizer with our train data, and get its vocabulary to tokenize our words\n",
    "train_X_tensor = tf.convert_to_tensor(df_train.iloc[:, 0])\n",
    "text_vectorizer = layers.TextVectorization(output_mode='int', standardize='lower', split='whitespace')\n",
    "text_vectorizer.adapt(train_X_tensor)\n",
    "vocabulario=text_vectorizer.get_vocabulary()\n",
    "\n",
    "# we process the three dataframes, so that we can use them to train, validate and test the model\n",
    "# the second parameter is the number of features that will be extracted from stack and buffer\n",
    "train_dataframe = process_traces(df_train, vocabulario, 2)\n",
    "val_dataframe = process_traces(df_val, vocabulario, 2)\n",
    "test_dataframe = process_traces(df_test, vocabulario, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create, train and test the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KuGA9Z55KHQz"
   },
   "outputs": [],
   "source": [
    "# We convert to tensor all the columns in our dataframes\n",
    "st_tensor = tf.convert_to_tensor(list(train_dataframe.iloc[:,0]))\n",
    "bt_tensor = tf.convert_to_tensor(list(train_dataframe.iloc[:,1]))\n",
    "sp_tensor = tf.convert_to_tensor(list(train_dataframe.iloc[:,2]))\n",
    "bp_tensor = tf.convert_to_tensor(list(train_dataframe.iloc[:,3]))\n",
    "ac_tensor = tf.convert_to_tensor(list(train_dataframe.iloc[:,4]))\n",
    "de_tensor = tf.convert_to_tensor(list(train_dataframe.iloc[:,5]))\n",
    "\n",
    "st_tensor_val = tf.convert_to_tensor(list(val_dataframe.iloc[:,0]))\n",
    "bt_tensor_val = tf.convert_to_tensor(list(val_dataframe.iloc[:,1]))\n",
    "sp_tensor_val = tf.convert_to_tensor(list(val_dataframe.iloc[:,2]))\n",
    "bp_tensor_val = tf.convert_to_tensor(list(val_dataframe.iloc[:,3]))\n",
    "ac_tensor_val = tf.convert_to_tensor(list(val_dataframe.iloc[:,4]))\n",
    "de_tensor_val = tf.convert_to_tensor(list(val_dataframe.iloc[:,5]))\n",
    "\n",
    "st_tensor_test = tf.convert_to_tensor(list(test_dataframe.iloc[:,0]))\n",
    "bt_tensor_test = tf.convert_to_tensor(list(test_dataframe.iloc[:,1]))\n",
    "sp_tensor_test = tf.convert_to_tensor(list(test_dataframe.iloc[:,2]))\n",
    "bp_tensor_test = tf.convert_to_tensor(list(test_dataframe.iloc[:,3]))\n",
    "ac_tensor_test = tf.convert_to_tensor(list(test_dataframe.iloc[:,4]))\n",
    "de_tensor_test = tf.convert_to_tensor(list(test_dataframe.iloc[:,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "RFWab1rI9F_Y"
   },
   "outputs": [],
   "source": [
    "# we define parameters for the network\n",
    "# if num_features is modified, then it should be modified above in the calls to process_traces\n",
    "num_features = 2\n",
    "num_pos = len(pos.values())\n",
    "num_deprels = len(deprel.values())\n",
    "num_actions = len(actions.values())\n",
    "num_words = len(vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "qSy_j-1M-Pj4"
   },
   "outputs": [],
   "source": [
    "# we define our model\n",
    "\n",
    "# Inputs\n",
    "# one input layer for each type of feature\n",
    "stack_tokens_input = layers.Input(shape=(num_features), name=\"stack_tokens_input\")\n",
    "buffer_tokens_input = layers.Input(shape=(num_features), name=\"buffer_tokens_input\")\n",
    "stack_pos_input = layers.Input(shape=(num_features), name=\"stack_pos_input\")\n",
    "buffer_pos_input = layers.Input(shape=(num_features), name=\"buffer_pos_input\")\n",
    "\n",
    "# Embedding\n",
    "# one embedding for each type of feature\n",
    "stack_tokens_embedding = layers.Embedding(num_words+1, 16, input_length=num_features, mask_zero=True, name=\"stack_tokens_embedding\")(stack_tokens_input)\n",
    "buffer_tokens_embedding = layers.Embedding(num_words+1, 16, input_length=num_features, mask_zero=True, name=\"buffer_tokens_embedding\")(buffer_tokens_input)\n",
    "stack_pos_embedding = layers.Embedding(num_pos+1, 16, input_length=num_features, mask_zero=True, name=\"stack_pos_embedding\")(stack_pos_input)\n",
    "buffer_pos_embedding = layers.Embedding(num_pos+1, 16, input_length=num_features, mask_zero=True, name=\"buffer_pos_embedding\")(buffer_pos_input)\n",
    "\n",
    "# concatenate the embeddings and flatten the result\n",
    "concatenate_embeddings = layers.Concatenate(axis=1, name=\"concatenate_embeddings\")([stack_tokens_embedding, buffer_tokens_embedding, stack_pos_embedding, buffer_pos_embedding])\n",
    "flat_embeddings = layers.Flatten(name=\"flat_embeddings\")(concatenate_embeddings)\n",
    "\n",
    "# Dense-Outputs\n",
    "# one dense layer for each desirable output\n",
    "action_dense = layers.Dense(num_actions+1, activation='softmax', name=\"action_dense\")(flat_embeddings)\n",
    "dependency_dense = layers.Dense(num_deprels+1, activation='softmax', name=\"dependency_dense\")(flat_embeddings)\n",
    "\n",
    "# Model\n",
    "model = keras.Model(inputs=[stack_tokens_input, buffer_tokens_input, stack_pos_input, buffer_pos_input],\n",
    "                     outputs=[action_dense, dependency_dense],\n",
    "                     name=\"dependency_parsing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DQAyI838GBq-",
    "outputId": "d64b9142-bb60-44cc-caa5-db1f0aedf3e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"dependency_parsing\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " stack_tokens_input (InputL  [(None, 2)]                  0         []                            \n",
      " ayer)                                                                                            \n",
      "                                                                                                  \n",
      " buffer_tokens_input (Input  [(None, 2)]                  0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " stack_pos_input (InputLaye  [(None, 2)]                  0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " buffer_pos_input (InputLay  [(None, 2)]                  0         []                            \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " stack_tokens_embedding (Em  (None, 2, 16)                100864    ['stack_tokens_input[0][0]']  \n",
      " bedding)                                                                                         \n",
      "                                                                                                  \n",
      " buffer_tokens_embedding (E  (None, 2, 16)                100864    ['buffer_tokens_input[0][0]'] \n",
      " mbedding)                                                                                        \n",
      "                                                                                                  \n",
      " stack_pos_embedding (Embed  (None, 2, 16)                288       ['stack_pos_input[0][0]']     \n",
      " ding)                                                                                            \n",
      "                                                                                                  \n",
      " buffer_pos_embedding (Embe  (None, 2, 16)                288       ['buffer_pos_input[0][0]']    \n",
      " dding)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_embeddings (Co  (None, 8, 16)                0         ['stack_tokens_embedding[0][0]\n",
      " ncatenate)                                                         ',                            \n",
      "                                                                     'buffer_tokens_embedding[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'stack_pos_embedding[0][0]', \n",
      "                                                                     'buffer_pos_embedding[0][0]']\n",
      "                                                                                                  \n",
      " flat_embeddings (Flatten)   (None, 128)                  0         ['concatenate_embeddings[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " action_dense (Dense)        (None, 6)                    774       ['flat_embeddings[0][0]']     \n",
      "                                                                                                  \n",
      " dependency_dense (Dense)    (None, 68)                   8772      ['flat_embeddings[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 211850 (827.54 KB)\n",
      "Trainable params: 211850 (827.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# compile and summarize the characteristics of the model\n",
    "\n",
    "model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer='adam',\n",
    "        metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PpJGLpJhFQne",
    "outputId": "b9c8d06d-111e-494b-910c-e0637a7f721a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2592/2592 [==============================] - 5s 2ms/step - loss: 1.5307 - action_dense_loss: 0.5511 - dependency_dense_loss: 0.9796 - action_dense_accuracy: 0.7986 - dependency_dense_accuracy: 0.7552 - val_loss: 1.0164 - val_action_dense_loss: 0.4162 - val_dependency_dense_loss: 0.6002 - val_action_dense_accuracy: 0.8431 - val_dependency_dense_accuracy: 0.8209\n",
      "Epoch 2/10\n",
      "2592/2592 [==============================] - 4s 2ms/step - loss: 0.8172 - action_dense_loss: 0.3426 - dependency_dense_loss: 0.4746 - action_dense_accuracy: 0.8737 - dependency_dense_accuracy: 0.8548 - val_loss: 0.9557 - val_action_dense_loss: 0.4141 - val_dependency_dense_loss: 0.5417 - val_action_dense_accuracy: 0.8474 - val_dependency_dense_accuracy: 0.8329\n",
      "Epoch 3/10\n",
      "2592/2592 [==============================] - 4s 2ms/step - loss: 0.6731 - action_dense_loss: 0.2884 - dependency_dense_loss: 0.3848 - action_dense_accuracy: 0.8962 - dependency_dense_accuracy: 0.8840 - val_loss: 0.9527 - val_action_dense_loss: 0.4291 - val_dependency_dense_loss: 0.5236 - val_action_dense_accuracy: 0.8464 - val_dependency_dense_accuracy: 0.8343\n",
      "Epoch 4/10\n",
      "2592/2592 [==============================] - 5s 2ms/step - loss: 0.5729 - action_dense_loss: 0.2534 - dependency_dense_loss: 0.3194 - action_dense_accuracy: 0.9106 - dependency_dense_accuracy: 0.9062 - val_loss: 0.9811 - val_action_dense_loss: 0.4537 - val_dependency_dense_loss: 0.5274 - val_action_dense_accuracy: 0.8423 - val_dependency_dense_accuracy: 0.8386\n",
      "Epoch 5/10\n",
      "2592/2592 [==============================] - 5s 2ms/step - loss: 0.4990 - action_dense_loss: 0.2284 - dependency_dense_loss: 0.2706 - action_dense_accuracy: 0.9204 - dependency_dense_accuracy: 0.9216 - val_loss: 1.0254 - val_action_dense_loss: 0.4828 - val_dependency_dense_loss: 0.5426 - val_action_dense_accuracy: 0.8368 - val_dependency_dense_accuracy: 0.8333\n",
      "Epoch 6/10\n",
      "2592/2592 [==============================] - 4s 2ms/step - loss: 0.4433 - action_dense_loss: 0.2097 - dependency_dense_loss: 0.2335 - action_dense_accuracy: 0.9280 - dependency_dense_accuracy: 0.9325 - val_loss: 1.0694 - val_action_dense_loss: 0.5087 - val_dependency_dense_loss: 0.5607 - val_action_dense_accuracy: 0.8366 - val_dependency_dense_accuracy: 0.8337\n",
      "Epoch 7/10\n",
      "2592/2592 [==============================] - 4s 2ms/step - loss: 0.4008 - action_dense_loss: 0.1956 - dependency_dense_loss: 0.2051 - action_dense_accuracy: 0.9324 - dependency_dense_accuracy: 0.9410 - val_loss: 1.1382 - val_action_dense_loss: 0.5413 - val_dependency_dense_loss: 0.5969 - val_action_dense_accuracy: 0.8345 - val_dependency_dense_accuracy: 0.8246\n",
      "Epoch 8/10\n",
      "2592/2592 [==============================] - 4s 2ms/step - loss: 0.3656 - action_dense_loss: 0.1836 - dependency_dense_loss: 0.1820 - action_dense_accuracy: 0.9374 - dependency_dense_accuracy: 0.9481 - val_loss: 1.1763 - val_action_dense_loss: 0.5643 - val_dependency_dense_loss: 0.6120 - val_action_dense_accuracy: 0.8318 - val_dependency_dense_accuracy: 0.8275\n",
      "Epoch 9/10\n",
      "2592/2592 [==============================] - 5s 2ms/step - loss: 0.3375 - action_dense_loss: 0.1741 - dependency_dense_loss: 0.1634 - action_dense_accuracy: 0.9415 - dependency_dense_accuracy: 0.9529 - val_loss: 1.2345 - val_action_dense_loss: 0.5934 - val_dependency_dense_loss: 0.6412 - val_action_dense_accuracy: 0.8265 - val_dependency_dense_accuracy: 0.8222\n",
      "Epoch 10/10\n",
      "2592/2592 [==============================] - 5s 2ms/step - loss: 0.3139 - action_dense_loss: 0.1659 - dependency_dense_loss: 0.1480 - action_dense_accuracy: 0.9434 - dependency_dense_accuracy: 0.9575 - val_loss: 1.3048 - val_action_dense_loss: 0.6319 - val_dependency_dense_loss: 0.6729 - val_action_dense_accuracy: 0.8195 - val_dependency_dense_accuracy: 0.8228\n"
     ]
    }
   ],
   "source": [
    "# train the model, with the validation set\n",
    "inputs=[st_tensor, bt_tensor, sp_tensor, bp_tensor]\n",
    "outputs=[ac_tensor, de_tensor]\n",
    "val_inputs=[st_tensor_val, bt_tensor_val, sp_tensor_val, bp_tensor_val]\n",
    "val_outputs=[ac_tensor_val, de_tensor_val]\n",
    "\n",
    "history=model.fit(x=inputs, y=outputs, epochs=10, validation_data=(val_inputs, val_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lw2aPyARKX08",
    "outputId": "f134e5c2-c711-43f4-981a-392fdb9a6019"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'action_dense_loss', 'dependency_dense_loss', 'action_dense_accuracy', 'dependency_dense_accuracy']\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 1.0488 - action_dense_loss: 0.5186 - dependency_dense_loss: 0.5302 - action_dense_accuracy: 0.8392 - dependency_dense_accuracy: 0.8554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0487929582595825,\n",
       " 0.5185903310775757,\n",
       " 0.5302026867866516,\n",
       " 0.8392366170883179,\n",
       " 0.8554198741912842]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the model\n",
    "\n",
    "test_inputs=[st_tensor_test, bt_tensor_test, sp_tensor_test, bp_tensor_test]\n",
    "test_outputs=[ac_tensor_test, de_tensor_test]\n",
    "\n",
    "print(model.metrics_names)\n",
    "model.evaluate(x=test_inputs, y=test_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run predictions for the test set, repair the trees and generate CONLLU evaluation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "-WkvCj6POR_-"
   },
   "outputs": [],
   "source": [
    "# We perform \"vertical\" prediction\n",
    "\n",
    "lote_estados = df_test.iloc[:, 1] # take a batch of states, in this case the whole dataset can be processed \n",
    "index = [i for i in range(len(df_test))] # create numerical ids from 0 to the lenght of the dataset\n",
    "estados = [initial_state(i) for i in lote_estados] # compute the initial states for the batch\n",
    "estados = list(zip(estados, index)) # zip together the state with the id, which references the row in the dataset\n",
    "items = [i for i in lote_estados] # take the items (the words and their features) for the batch\n",
    "\n",
    "# as we did before, build (for each sentence of the batch) an array of words, and the corresponding array of tokenized words\n",
    "words = [[items[it][k][1] for k in range(len(items[it]))] for it in range(len(items))] \n",
    "vectorized_words = [[vocabulario.index(words[wor][w].lower()) if words[wor][w].lower() in vocabulario else vocabulario.index(words[wor][w]) if words[wor][w] in vocabulario else 1 for w in range(1,len(words[wor]))] for wor in range(len(words))]\n",
    "for i in range(len(vectorized_words)): # add the ROOT token\n",
    "  vectorized_words[i].insert(0, len(text_vectorizer.get_vocabulary()))\n",
    "\n",
    "# array to store the final, predicted trees\n",
    "predicted_trees=[]\n",
    "\n",
    "# counter to control the infinite loop\n",
    "count = 0\n",
    "\n",
    "# loop infinitely\n",
    "while True:\n",
    "  stack_list = [] # lists for the different input features of each state\n",
    "  buffer_list = []\n",
    "  stack_pos_list = []\n",
    "  buffer_pos_list = []\n",
    "  finish_indexes=[] # list to store indexes of sentences whose processing has finished\n",
    "\n",
    "  for i in range(len(estados)): # for each state\n",
    "    if final_state(estados[i][0]): # if we're done with this sentence, store its index and pass to the next one\n",
    "      finish_indexes.append(i)\n",
    "      continue\n",
    "    # extract the features of this state and store them on the lists\n",
    "    stack, buffer, stack_pos, buffer_pos = extract_features(items[i], estados[i][0], vectorized_words[i], num_features, pos)\n",
    "    stack_list.append(stack)\n",
    "    buffer_list.append(buffer)\n",
    "    stack_pos_list.append(stack_pos)\n",
    "    buffer_pos_list.append(buffer_pos)\n",
    "\n",
    "  # we reverse the ordering of the indexes of finished sentences (to not mess up the others while deleting) and for each one\n",
    "  for index in sorted(finish_indexes, reverse=True):\n",
    "    # we store the id (row of original dataset) and the arcs\n",
    "    predicted_trees.append((estados[index][1], estados[index][0][3]))\n",
    "    # we delete this sentence from everywhere\n",
    "    del estados[index]\n",
    "    del items[index]\n",
    "    del vectorized_words[index]\n",
    "\n",
    "  # if we end up having no states left, we break the infinite loop\n",
    "  if not estados:\n",
    "    break\n",
    "\n",
    "  # we transform our feature lists to tensors and use them as inputs to get the current set of predictions\n",
    "  net_inputs = [tf.convert_to_tensor(stack_list), tf.convert_to_tensor(buffer_list), tf.convert_to_tensor(stack_pos_list), tf.convert_to_tensor(buffer_pos_list)]\n",
    "  action_pred, deprel_pred = model.predict(x=net_inputs, verbose=0)\n",
    "\n",
    "  # for each state, and according to the predictions, we apply the action/dependency and update the state (keeping the same id) \n",
    "  for i in range(len(estados)):\n",
    "    action, state = check_action_and_state(deprel_pred[i], action_pred[i], estados[i][0], inv_deprel)\n",
    "    estados[i] = (state, estados[i][1])\n",
    "\n",
    "  # we count one more step, and once we overtake a thousand, we break out of the infinite loop\n",
    "  count = count + 1\n",
    "  if count > 1000:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "KhVXmbTVhNDv"
   },
   "outputs": [],
   "source": [
    "# we create a deep copy of our test dataframe\n",
    "df_test_predicted = pd.DataFrame(data = copy.deepcopy(df_test.values), columns = df_test.columns)\n",
    "\n",
    "# and for each tree that we previously predicted, we repair its possible errors and update the new dataframe with its\n",
    "# arc information\n",
    "for tree in predicted_trees:\n",
    "  repair_tree(tree, df_test)\n",
    "  for arc in tree[1]:\n",
    "    df_test_predicted.iloc[tree[0], 1][arc[2]][6] = arc[0]\n",
    "    df_test_predicted.iloc[tree[0], 1][arc[2]][7] = arc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Jn5T3WaOkcS0"
   },
   "outputs": [],
   "source": [
    "# this writes the prediction dataset to a file in the CONLLU format\n",
    "\n",
    "with open(\"./datasets/generated/en_test_predictions_c4.conllu\", 'a', encoding=\"utf-8\") as f:\n",
    "  for item in df_test_predicted['items']:\n",
    "    for word in item:\n",
    "      if word[0] == 0:\n",
    "        continue\n",
    "      f.write('\\t'.join(str(el) for el in word)+'\\n')\n",
    "    f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "eyiO7bQdn6gI"
   },
   "outputs": [],
   "source": [
    "# this writes our test dataframe (the one which is clean, with non-projective sentences removed) to a file in the CONLLU format\n",
    "\n",
    "with open(\"./datasets/generated/en_test_clean.conllu\", 'a', encoding=\"utf-8\") as f:\n",
    "  for item in df_test['items']:\n",
    "    for word in item:\n",
    "      if word[0] == 0:\n",
    "        continue\n",
    "      f.write('\\t'.join(str(el) for el in word)+'\\n')\n",
    "    f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
