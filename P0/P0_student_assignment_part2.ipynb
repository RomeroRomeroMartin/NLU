{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cddFHyPuhhtu",
   "metadata": {
    "id": "cddFHyPuhhtu"
   },
   "source": [
    "# P0 Student assignment: Simple models with Keras\n",
    "\n",
    "**Goal**: implement **three models** for multiclass text classification on the [Women's E-commerce clothing reviews](https://github.com/ya-stack/Women-s-Ecommerce-Clothing-Reviews) [dataset](https://www.kaggle.com/datasets/nicapotato/womens-ecommerce-clothing-reviews), two of them simple feed-forward models using a `Tokenizer` and `TextVectorizer`, respectively, and the third a Convolutional Neural Network (CNN) using a `TextVectorizer` layer and embeddings. \n",
    "\n",
    "**Teams**: one person or two --> **Martín Romero Romero and Pablo Miguel Pérez Ferreiro**.\n",
    "\n",
    "**Due date**: October 4, 2023.\n",
    "\n",
    "### 1. Data preparation\n",
    "\n",
    "The first step is to downlad the dataset (a `csv` file) from *GitHub*. Suggestions:\n",
    "- You can use the utility function [`tensorflow.keras.utils.get_file()`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file) to download the file. You should set an absolute path to save the file, taking into account that, in *Google Colaboratory*, you have direct acces to the folder `/content/`.\n",
    "- There are many ways to load a `csv` in memory. One simple way is to use `csv.reader()`.\n",
    "\n",
    "~~~\n",
    "with open(path, newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "~~~\n",
    "\n",
    "The resulting data estructure (`data`) is a python list of lists (the reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d51e5c7b-4250-49f6-a227-f26331a42052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 19:09:39.539841: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-18 19:09:39.663177: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-18 19:09:39.666539: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/martin/catkin_ws/devel/lib:/opt/ros/noetic/lib:/usr/local/cuda-12.0/lib64:/usr/local/cuda-12.0/lib64:/usr/local/cuda-12.0-/lib64:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins\n",
      "2023-10-18 19:09:39.666552: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-10-18 19:09:40.355353: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/martin/catkin_ws/devel/lib:/opt/ros/noetic/lib:/usr/local/cuda-12.0/lib64:/usr/local/cuda-12.0/lib64:/usr/local/cuda-12.0-/lib64:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins\n",
      "2023-10-18 19:09:40.355406: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/martin/catkin_ws/devel/lib:/opt/ros/noetic/lib:/usr/local/cuda-12.0/lib64:/usr/local/cuda-12.0/lib64:/usr/local/cuda-12.0-/lib64:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins\n",
      "2023-10-18 19:09:40.355411: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/martin/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# if needed (assumes that pip is installed)\n",
    "# !pip install tensorflow==2.13.0\n",
    "# !pip install pandas\n",
    "# !pip install scikit-learn\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a952b98b-2ece-4b83-a485-5f6b21a86917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9227adb7",
   "metadata": {
    "id": "9227adb7"
   },
   "outputs": [],
   "source": [
    "# ToDo:\n",
    "# load the dataset\n",
    "# load and preprocess\n",
    "\n",
    "# download (the path must be specified as absolute, so this piece is not really portable)\n",
    "tf.keras.utils.get_file(fname='/home/martin/Escritorio/NLU/P0/reviews.csv', origin='https://raw.githubusercontent.com/ya-stack/Women-s-Ecommerce-Clothing-Reviews/master/Womens%20Clothing%20E-Commerce%20Reviews.csv')    \n",
    "\n",
    "# read and drop the unnecesary columns\n",
    "data = pd.read_csv('reviews.csv')\n",
    "data.drop(['Unnamed: 0', 'Clothing ID', 'Age', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name'], axis=1, inplace=True)\n",
    "# replace null contents with empty strings\n",
    "data['Title'].fillna('', inplace=True)\n",
    "data['Review Text'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DsObjWyPkQo0",
   "metadata": {
    "id": "DsObjWyPkQo0"
   },
   "source": [
    "Once you have the rows of the `csv` file in a data structure (remember that the first one is the names of the attributes of the data set, and must be discarded) you have to preprocess the data for its use as an input to the neural networks:\n",
    "1. Extract the textual data from the rows, included in the fields `Title` and `Review Text`, and join both fields if `Title` is not empty.\n",
    "2. Convert the field `Rating`, whose content are integers in the interval [1,5] into three classes: negative (ratings 1,2), neutral (rating 3) and positive (ratings 4,5).\n",
    "3. The dataset contains about 23,000 reviews. Reserve the first 18,000 for training, and the rest for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "JOIPjJMKhcbt",
   "metadata": {
    "id": "JOIPjJMKhcbt"
   },
   "outputs": [],
   "source": [
    "# ToDo:\n",
    "# carry out the required preprocessing on Title and Review Text and remove the header\n",
    "# transform the rating 1-5 to three categories: negative:0, neutral:1, positive:2\n",
    "# obtain train and validation sets.\n",
    "\n",
    "# function to convert the ratings to categories\n",
    "def convert_rating(rating):\n",
    "    if rating <= 2:\n",
    "        return 0\n",
    "    elif rating == 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# create new field with all the textual content, and drop no longer needed fields\n",
    "data['text']=data['Title']+' '+data['Review Text']\n",
    "data.drop(['Title', 'Review Text'], axis=1, inplace=True)\n",
    "# likewise with the converted ratings\n",
    "data['sentiment'] = data['Rating'].apply(convert_rating)\n",
    "data.drop(['Rating'], axis=1, inplace=True)\n",
    "data=data[['text', 'sentiment']]\n",
    "# split the dataset between train and test, and subsequently split those between target and predictors\n",
    "train_data, val_data = train_test_split(data, train_size=18000, random_state=42)\n",
    "train_X, train_y = train_data.iloc[:,0], train_data.iloc[:,1]\n",
    "val_X, val_y = val_data.iloc[:,0], val_data.iloc[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MeRFaU6MnCpy",
   "metadata": {
    "id": "MeRFaU6MnCpy"
   },
   "source": [
    "### 2. Perceptron with Tokenizer.\n",
    "\n",
    "In the first model, you are going to use a `Tokenizer()` object to process the training and validation texts, transforming each review into binary vectors (of length *n*, where *n* is the size of the vocabulary) in which the positions of the words appearing in the review will be coded as `1` (clue: you can use the method `texts_to_matrix()` for this). You can set a maximum size for the vocabulary (parameter `num_words`), but it is not necessary.\n",
    "\n",
    "Remember that you have to use the `fit_on_texts()` method in order to build the vocabulary of the tokenizer from the training data.\n",
    "\n",
    "In addition, you have to convert vectors with the labels (negative=0, neutral=1, positive=2) from the training and validation sets to a data type which make possible to use them with the loss function [`categorical_crossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical) (clue: you may want to use the utility function [`tensorflow.keras.utils.to_categorical()`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "z4XZoZSBM6JQ",
   "metadata": {
    "id": "z4XZoZSBM6JQ"
   },
   "outputs": [],
   "source": [
    "# ToDo:\n",
    "# create the tokenizer and build the vocabulary/word index\n",
    "# obtain a vectorized representation for train and validation sets using Tokenizer\n",
    "\n",
    "# create the tokenizer and fit with training textual data, building the vocabulary\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_X)\n",
    "# vectorize the textual data in both sets using the fitted tokenizer\n",
    "train_X_oh = tokenizer.texts_to_matrix(train_X, mode='binary')\n",
    "val_X_oh = tokenizer.texts_to_matrix(val_X, mode='binary')\n",
    "# one-hot encode the target variables to allow the use of categorical_crossentropy\n",
    "train_y_oh=tf.keras.utils.to_categorical(train_y)\n",
    "val_y_oh=tf.keras.utils.to_categorical(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9063addb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_oh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nBPuYWUDrx6J",
   "metadata": {
    "id": "nBPuYWUDrx6J"
   },
   "source": [
    "Now it is time to create the `Sequential` architecture of out first model. In this case, a simple perceptron with three layers (input, hidden, output) will suffice. A few pointers:\n",
    "- You will need to set the `input_shape` of the first layer of the network to the size of the vocabulary in the `Tokenizer`.\n",
    "- The number of units and the activation function in the output layer must be appropiate for a three-class classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ec70f68",
   "metadata": {
    "id": "5ec70f68"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 19:09:44.491131: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-18 19:09:44.491391: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/martin/catkin_ws/devel/lib:/opt/ros/noetic/lib:/usr/local/cuda-12.0/lib64:/usr/local/cuda-12.0/lib64:/usr/local/cuda-12.0-/lib64:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins\n",
      "2023-10-18 19:09:44.491480: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/martin/catkin_ws/devel/lib:/opt/ros/noetic/lib:/usr/local/cuda-12.0/lib64:/usr/local/cuda-12.0/lib64:/usr/local/cuda-12.0-/lib64:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins\n",
      "2023-10-18 19:09:44.491534: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/martin/catkin_ws/devel/lib:/opt/ros/noetic/lib:/usr/local/cuda-12.0/lib64:/usr/local/cuda-12.0/lib64:/usr/local/cuda-12.0-/lib64:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins\n",
      "2023-10-18 19:09:44.491593: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/martin/catkin_ws/devel/lib:/opt/ros/noetic/lib:/usr/local/cuda-12.0/lib64:/usr/local/cuda-12.0/lib64:/usr/local/cuda-12.0-/lib64:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins\n",
      "2023-10-18 19:09:44.561157: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/martin/catkin_ws/devel/lib:/opt/ros/noetic/lib:/usr/local/cuda-12.0/lib64:/usr/local/cuda-12.0/lib64:/usr/local/cuda-12.0-/lib64:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins\n",
      "2023-10-18 19:09:44.561791: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-10-18 19:09:44.562604: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# ToDo:\n",
    "# Create a NN model: a simple perceptron\n",
    "\n",
    "# define the model using the sequential API: input, hidden, output\n",
    "model=tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=(len(tokenizer.word_index)+1,))) # length of the word index, plus one for index 0\n",
    "model.add(tf.keras.layers.Dense(24, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(3, activation='softmax')) # three output neurons for the three classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D7zWtsGDsyY2",
   "metadata": {
    "id": "D7zWtsGDsyY2"
   },
   "source": [
    "Now compile and train the model. You can use any optimizer you want, but the loss function must be [`categorical_crossentropy`](https://keras.io/api/losses/probabilistic_losses/#categorical_crossentropy-function), the metric used will be `accuracy`, and you will provide the validation sets for the computation of the validation loss and validation accuracy at the end of each epoch of training, with the argument `validation_data`.\n",
    "\n",
    "The model will train for 10 epochs.\n",
    "\n",
    "Expect a validation accuracy of 0.80-0.83, approximamtely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93458fd7",
   "metadata": {
    "id": "93458fd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "282/282 [==============================] - 2s 5ms/step - loss: 0.5710 - accuracy: 0.7731 - val_loss: 0.4626 - val_accuracy: 0.7667\n",
      "Epoch 2/10\n",
      "282/282 [==============================] - 1s 3ms/step - loss: 0.3734 - accuracy: 0.8451 - val_loss: 0.4065 - val_accuracy: 0.8288\n",
      "Epoch 3/10\n",
      "282/282 [==============================] - 1s 3ms/step - loss: 0.2964 - accuracy: 0.8857 - val_loss: 0.4132 - val_accuracy: 0.8285\n",
      "Epoch 4/10\n",
      "282/282 [==============================] - 1s 3ms/step - loss: 0.2448 - accuracy: 0.9109 - val_loss: 0.4349 - val_accuracy: 0.8279\n",
      "Epoch 5/10\n",
      "282/282 [==============================] - 1s 3ms/step - loss: 0.2014 - accuracy: 0.9308 - val_loss: 0.4752 - val_accuracy: 0.8212\n",
      "Epoch 6/10\n",
      "282/282 [==============================] - 1s 3ms/step - loss: 0.1629 - accuracy: 0.9469 - val_loss: 0.4914 - val_accuracy: 0.8172\n",
      "Epoch 7/10\n",
      "282/282 [==============================] - 1s 3ms/step - loss: 0.1308 - accuracy: 0.9603 - val_loss: 0.5552 - val_accuracy: 0.8203\n",
      "Epoch 8/10\n",
      "282/282 [==============================] - 1s 3ms/step - loss: 0.1045 - accuracy: 0.9708 - val_loss: 0.5738 - val_accuracy: 0.8152\n",
      "Epoch 9/10\n",
      "282/282 [==============================] - 1s 3ms/step - loss: 0.0831 - accuracy: 0.9791 - val_loss: 0.6163 - val_accuracy: 0.8155\n",
      "Epoch 10/10\n",
      "282/282 [==============================] - 1s 3ms/step - loss: 0.0673 - accuracy: 0.9842 - val_loss: 0.6575 - val_accuracy: 0.8150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd06c60e940>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ToDo: \n",
    "# Train your model here\n",
    "\n",
    "# compile and fit the model\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.fit(train_X_oh, train_y_oh, epochs=10, validation_data=(val_X_oh, val_y_oh), batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yd_GU74Qt00D",
   "metadata": {
    "id": "yd_GU74Qt00D"
   },
   "source": [
    "*¿Does the validation accuracy grow with each epoch?*\n",
    "\n",
    "No. It grows during the first few epochs, but quickly stagnates or even decreases. In fact, the best validation accuracy achieved is around 83% (82.8), yet the validation accuracy during the tenth epoch is only of 80.73%. What does grow on each epoch is the training accuracy, but that is far less interesting because it can easily mean that the model is overfitting to the examples. This last hypothesis is reinforced by that fact that the validation accuracy falls: our model is progressively getting better at recognizing the textual patterns of the training set, but losing generalization potency and thus performing worse on the validation set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A_dNVEIevs25",
   "metadata": {
    "id": "A_dNVEIevs25"
   },
   "source": [
    "### 3. Perceptron with a TextVectorizer layer.\n",
    "\n",
    "Now you are going to implement a new neural network, with two differences with respect to the previous one:\n",
    "- We will use a [`TextVectorizer`](https://keras.io/api/layers/preprocessing_layers/core_preprocessing_layers/text_vectorization/) Layer instead of a `Tokenizer`.\n",
    "- The loss function will be [`sparse_categorical_crossentropy`](https://keras.io/api/losses/probabilistic_losses/#sparse_categorical_crossentropy-function).\n",
    "\n",
    "Your first task is to set the `TextVectorization` layer. Remember you have to create the layer and call the method `adapt()` on the training data before adding the layer to the new model. You can use the default values when creating the layer if you wish, except for `output_mode` that has to be set to `'multi_hot'`, so a binary vector the size of the vocabulary is generated for each example, as `Tokenizer` did in the first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac19c962",
   "metadata": {
    "id": "ac19c962"
   },
   "outputs": [],
   "source": [
    "# ToDo:\n",
    "# Prepare your text vectorizer layer\n",
    "\n",
    "# let's transform our dataframes to tensors, so that they can be used with TextVectorizer\n",
    "train_X_tensor=tf.convert_to_tensor(train_X)\n",
    "val_X_tensor=tf.convert_to_tensor(val_X)\n",
    "train_y_tensor=tf.convert_to_tensor(train_y)\n",
    "val_y_tensor=tf.convert_to_tensor(val_y)\n",
    "\n",
    "# create the vectorizer and train it with the training textual data.\n",
    "text_vectorizer = layers.TextVectorization(output_mode='multi_hot')\n",
    "text_vectorizer.adapt(train_X_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O9oKPPNo1Skr",
   "metadata": {
    "id": "O9oKPPNo1Skr"
   },
   "source": [
    "Now you can create the your second `Sequential` model, adding its layers one by one. Obviously, the previously created `TextVectorizer` goes first. There is not need to define an input layer. You can add the rest of the layers after the text vectorizer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "Geg_avqn1T3E",
   "metadata": {
    "id": "Geg_avqn1T3E"
   },
   "outputs": [],
   "source": [
    "# ToDo:\n",
    "# create the model2 in a similar way to the first model but with the text vectorization layer\n",
    "\n",
    "# same model as before, only with text_vectorizer first\n",
    "model = tf.keras.Sequential()\n",
    "model.add(text_vectorizer)\n",
    "model.add(tf.keras.layers.Dense(24, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mqLJD4p343SP",
   "metadata": {
    "id": "mqLJD4p343SP"
   },
   "source": [
    "Once the topology of the new model is set, you will set the datasets, compile and train it. Important:\n",
    "\n",
    "- Remember that you are supposed to use `sparse_categorical_crossentropy`, so the label vectors for both training and validation will have to be of the appropiate type and dimensions.\n",
    "- `TextVectorizer` accepts its training input in batches. That means you will have to change the way training data is passed to the model. One way to do it is to create an object `Tensorflow.Dataset` and organize it in batches, using the methods `tensorflow.Dataset.from_tensor_slices()` and \n",
    "`tensorflow.Dataset.from_tensor_slices.batch()`\n",
    "\n",
    "~~~\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "~~~\n",
    "\n",
    "where `x` and `y` are the training samples and labels, and `batch_size` is a number. You can do the same with the validation data, but it is not mandatory. When training the model, you have to pass the dataset instead of x and y, and **omit** the `batch_size` parameter, since it is already set in the dataset.\n",
    "\n",
    "~~~\n",
    "model.fit(train_ds,...,epochs=10)\n",
    "~~~\n",
    "\n",
    "You can use whichever optimizer you prefer, but you will use accuracy to measure the performance of the model, provide the validation data through the argument `validation_data`, and train for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "LwD3TBL-cFfz",
   "metadata": {
    "id": "LwD3TBL-cFfz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "282/282 [==============================] - 2s 5ms/step - loss: 0.5535 - accuracy: 0.7824 - val_loss: 0.4434 - val_accuracy: 0.8106\n",
      "Epoch 2/10\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.3554 - accuracy: 0.8592 - val_loss: 0.4108 - val_accuracy: 0.8230\n",
      "Epoch 3/10\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.2709 - accuracy: 0.9004 - val_loss: 0.4261 - val_accuracy: 0.8236\n",
      "Epoch 4/10\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.2162 - accuracy: 0.9260 - val_loss: 0.4562 - val_accuracy: 0.8248\n",
      "Epoch 5/10\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.1728 - accuracy: 0.9444 - val_loss: 0.4930 - val_accuracy: 0.8215\n",
      "Epoch 6/10\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.1383 - accuracy: 0.9591 - val_loss: 0.5321 - val_accuracy: 0.8206\n",
      "Epoch 7/10\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.1116 - accuracy: 0.9684 - val_loss: 0.5764 - val_accuracy: 0.8192\n",
      "Epoch 8/10\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.0910 - accuracy: 0.9758 - val_loss: 0.6196 - val_accuracy: 0.8184\n",
      "Epoch 9/10\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.0748 - accuracy: 0.9818 - val_loss: 0.6621 - val_accuracy: 0.8139\n",
      "Epoch 10/10\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.0627 - accuracy: 0.9854 - val_loss: 0.7068 - val_accuracy: 0.8099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcfc0130ca0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ToDo:\n",
    "# train your model2 after having compiled and prepared the training input in batches as suggested in the instructions\n",
    "\n",
    "# prepare our datasets from the tensors obtained before, and define their batches\n",
    "train_ds=tf.data.Dataset.from_tensor_slices((train_X_tensor, train_y_tensor))\n",
    "train_ds=train_ds.batch(64)\n",
    "val_ds=tf.data.Dataset.from_tensor_slices((val_X_tensor, val_y_tensor))\n",
    "val_ds=val_ds.batch(64)\n",
    "\n",
    "# with sparse_categorical_crossentropy, there is no need to one-hot encode the labels, integers are alright\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.fit(train_ds, epochs=10, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oP13PmGp5c6H",
   "metadata": {
    "id": "oP13PmGp5c6H"
   },
   "source": [
    "*¿Is the new model any better than the previous one?*\n",
    "\n",
    "The model seems to be performing very similarly to the previous one. Both achieve very high training accuracy at the completion of the ten epochs, with far less impressive (although still very good) validation accuracy, and the evolution of their validation accuracy even follows a similar pattern, with increases at the first three epochs which are followed by a progressive descent until landing around the mark of 80.8%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90UD8dU8h_Bt",
   "metadata": {
    "id": "90UD8dU8h_Bt"
   },
   "source": [
    "### 4. CNN with TextVectorizer layer and word embeddings\n",
    "\n",
    "Finally, you are going to train a third model with the following components:\n",
    "- A `TextVectorizer` layer.\n",
    "- An `Embedding` layer.\n",
    "- One or more `Conv1D` layers.\n",
    "- A `GlobalMaxPooling1D` layer.\n",
    "- One or more `Dense` layers for the computation of results.\n",
    "- A output layer with the appropiate activation function for a multiclass classifier.\n",
    "\n",
    "You will use the functional API. See the doc reference [here](https://keras.io/guides/functional_api/).\n",
    "\n",
    "Our goal is to process the input texts token by token using a Convolutional Neural Network (CNN) and embeddings. The first step is to define the `TextVectorizer` layer. This time the output of this layer will be a vector of integer numbers (the input for the [`Embedding`](https://keras.io/api/layers/core_layers/embedding/) layer), with one integer for each token in the input text, so `output_mode` must be set to `int` or omitted (since `int` is the default value for this parameter). In addition, all sequences of integers (words) given to the embedding layer must have the same length. To ensure that, you will use the parameter `output_sequence_length` in the definition of the `TextVectorizer` (i.e. `output_sequence_length=100`). That will cut sequences longer than the value of `output_sequence_length` and pad shorter ones with zeros.\n",
    "\n",
    "Once the layer is defined, it will must be trained with the method `adapt()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "LYNKH3QRhln2",
   "metadata": {
    "id": "LYNKH3QRhln2"
   },
   "outputs": [],
   "source": [
    "# ToDo:\n",
    "# Create the text vectorization layer...\n",
    "\n",
    "# we will repeat the tensors/dataset preprocessing used before here; the code is duplicated to allow flexibility in the order of execution\n",
    "train_X_tensor=tf.convert_to_tensor(train_X)\n",
    "val_X_tensor=tf.convert_to_tensor(val_X)\n",
    "train_y_tensor=tf.convert_to_tensor(train_y)\n",
    "val_y_tensor=tf.convert_to_tensor(val_y)\n",
    "train_ds=tf.data.Dataset.from_tensor_slices((train_X_tensor, train_y_tensor))\n",
    "train_ds=train_ds.batch(64)\n",
    "val_ds=tf.data.Dataset.from_tensor_slices((val_X_tensor, val_y_tensor))\n",
    "val_ds=val_ds.batch(64)\n",
    "\n",
    "# now create and train the text vectorizer\n",
    "text_vectorizer = layers.TextVectorization(output_mode='int', output_sequence_length=100)\n",
    "text_vectorizer.adapt(train_X_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bTzl5vn7jRg-",
   "metadata": {
    "id": "bTzl5vn7jRg-"
   },
   "source": [
    "You have to start the defintion of the model with an `Input` layer, e.g.:\n",
    "\n",
    "~~~\n",
    "inputs = tf.keras.Input(shape=(1,),dtype=tf.string)\n",
    "~~~\n",
    "\n",
    "then you can add the `TextVectorizer`, `Conv1D`, ... layers.\n",
    "\n",
    "The `Embedding` layer has at least two parameters: the size of the vocabulary and the size of the embeddings. For the vocabulary you have two choices: set it in avance when creating the layer, via the `max_tokens` parameter, or to let all tokens of the training set be part of the vocabulary. In the latter case, you can get the vocabulary size from the layer, using the method `vocabulary_size()`.\n",
    "\n",
    "You must the set embedding dimension to a integer value, e.g. `30`.\n",
    "\n",
    "In the `Conv1D` layer you have to set two parameters, `filters` and `kernel_size`. Both are integers. The first can have any integer value (e.g., `64`of `128`) but the higher is set, the bigger the number of computations will be, while the second should be small compared to the length of the sequences of words (e.g. `3` or `5`).\n",
    "\n",
    "We finish with the output layer:\n",
    "\n",
    "~~~\n",
    "outputs = tf.keras.layers.Dense(...)(x)\n",
    "~~~\n",
    "\n",
    "where `x` is the output of the previous layer. At this point we can define the model:\n",
    "\n",
    "~~~\n",
    "model_functional = keras.Model(inputs=inputs, outputs=outputs)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5DPqbxMwjVpE",
   "metadata": {
    "id": "5DPqbxMwjVpE"
   },
   "outputs": [],
   "source": [
    "# ToDo:\n",
    "# create your model3... for example:\n",
    "\n",
    "# using the functional API, we define the model described above\n",
    "inputs = tf.keras.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = layers.Embedding(text_vectorizer.vocabulary_size(), 30)(x)\n",
    "x = layers.Conv1D(filters=64, kernel_size=5)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "outputs = layers.Dense(3, activation='softmax')(x)\n",
    "model = models.Model(inputs=inputs, outputs=outputs, name=\"sentiment_analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NVUVLS6QoU9Z",
   "metadata": {
    "id": "NVUVLS6QoU9Z"
   },
   "source": [
    "You can use exactly the same datasets than in the previous model for training and validation, and the optimizer of you preference, but you will use accuracy as performance metric and sparse categorical crossentropy as loss function, provide the validation data through the argument `validation_data`, and train the model for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "PinlDFImk4Ma",
   "metadata": {
    "id": "PinlDFImk4Ma"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "282/282 [==============================] - 2s 7ms/step - loss: 0.6051 - accuracy: 0.7843 - val_loss: 0.4758 - val_accuracy: 0.7971\n",
      "Epoch 2/10\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.4103 - accuracy: 0.8284 - val_loss: 0.4311 - val_accuracy: 0.8152\n",
      "Epoch 3/10\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.3445 - accuracy: 0.8568 - val_loss: 0.4270 - val_accuracy: 0.8201\n",
      "Epoch 4/10\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.2949 - accuracy: 0.8817 - val_loss: 0.4354 - val_accuracy: 0.8241\n",
      "Epoch 5/10\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.2498 - accuracy: 0.9060 - val_loss: 0.4538 - val_accuracy: 0.8248\n",
      "Epoch 6/10\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.2063 - accuracy: 0.9273 - val_loss: 0.4815 - val_accuracy: 0.8265\n",
      "Epoch 7/10\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.1653 - accuracy: 0.9505 - val_loss: 0.5162 - val_accuracy: 0.8210\n",
      "Epoch 8/10\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.1282 - accuracy: 0.9666 - val_loss: 0.5547 - val_accuracy: 0.8190\n",
      "Epoch 9/10\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.0975 - accuracy: 0.9775 - val_loss: 0.5908 - val_accuracy: 0.8168\n",
      "Epoch 10/10\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.0735 - accuracy: 0.9863 - val_loss: 0.6282 - val_accuracy: 0.8161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcfac31a4c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ToDo: \n",
    "# Train your model3 \n",
    "\n",
    "# compile and train\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.fit(train_ds, epochs=10, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9nI7Zestolma",
   "metadata": {
    "id": "9nI7Zestolma"
   },
   "source": [
    "*¿Does the new model perform any better than the previous two?*\n",
    "\n",
    "It performs slightly better than both, although the difference is probably not significant, as at the last epoch the validation accuracies are 81.1% vs 80.8%. The evolution of said metric is a bit more stable than on the previous models, though, and it takes considerably longer to start degrading (and it degrades slower). It's important to note that we've chosen to go with the 'bare-minimum' model, according to the instructions of the assignment; but it's possible that including some of the additional layers mentioned on the instructions (i.e. more convolutional or dense layers) could help make this model a bit better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723368ab-ce87-4efd-b745-ffa137531919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
