{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 20:03:02.290980: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-22 20:03:02.372463: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-22 20:03:02.374858: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-11-22 20:03:02.374867: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-11-22 20:03:02.795398: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-11-22 20:03:02.795433: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-11-22 20:03:02.795437: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from preprocess import *\n",
    "from tagger import POS_Tagger\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three datasets used during our tests are accessible on the following GitHub repositories:\n",
    "- English dataset: https://github.com/UniversalDependencies/UD_English-EWT\n",
    "- French dataset: https://github.com/UniversalDependencies/UD_French-GSD\n",
    "- Italian dataset: https://github.com/UniversalDependencies/UD_Italian-ISDT\n",
    "\n",
    "Below there is code to download the three sets of each dataset (train, dev/validation, test) to your computer. Feel free to comment/uncomment any lines to decide on what datasets you actually download (but keep in mind that you must do the same for the rest of the preprocessing steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(url, name):\n",
    "    filename = os.path.join(os.getcwd(), name) #we save on the working directory\n",
    "\n",
    "    r = requests.get(url) #get the webpage\n",
    "    with open(filename, 'w', encoding=\"utf-8\") as f: #and write it to a file\n",
    "      f.write(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English treebanks\n",
    "get_dataset(\"https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu\", \"en_ewt-ud-train.conllu\")\n",
    "get_dataset(\"https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-dev.conllu\", \"en_ewt-ud-dev.conllu\")\n",
    "get_dataset(\"https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-test.conllu\", \"en_ewt-ud-test.conllu\")\n",
    "\n",
    "# French treebanks\n",
    "get_dataset(\"https://raw.githubusercontent.com/UniversalDependencies/UD_French-GSD/master/fr_gsd-ud-train.conllu\", \"fr_gsd-ud-train.conllu\")\n",
    "get_dataset(\"https://raw.githubusercontent.com/UniversalDependencies/UD_French-GSD/master/fr_gsd-ud-dev.conllu\", \"fr_gsd-ud-dev.conllu\")\n",
    "get_dataset(\"https://raw.githubusercontent.com/UniversalDependencies/UD_French-GSD/master/fr_gsd-ud-test.conllu\", \"fr_gsd-ud-test.conllu\")\n",
    "\n",
    "# Italian treebanks\n",
    "get_dataset(\"https://raw.githubusercontent.com/UniversalDependencies/UD_Italian-ISDT/master/it_isdt-ud-train.conllu\", \"it_isdt-ud-train.conllu\")\n",
    "get_dataset(\"https://raw.githubusercontent.com/UniversalDependencies/UD_Italian-ISDT/master/it_isdt-ud-dev.conllu\", \"it_isdt-ud-dev.conllu\")\n",
    "get_dataset(\"https://raw.githubusercontent.com/UniversalDependencies/UD_Italian-ISDT/master/it_isdt-ud-test.conllu\", \"it_isdt-ud-test.conllu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocess the datasets downloaded, essentially stripping them from comments, multiwords, and empty tokens. This generates new .conllu files, that we'll use next to produce our data structures holding inputs and targets of the PoS Tagger. As said, feel free to comment/uncomment any lines referring to datasets you don't/do have.\n",
    "\n",
    "This makes use of the *preprocess_dataset* function, defined in the preprocess.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the english treebanks\n",
    "preprocess_dataset(\"en_ewt-ud-train.conllu\", \"my_en_train.conllu\")\n",
    "preprocess_dataset(\"en_ewt-ud-dev.conllu\", \"my_en_dev.conllu\")\n",
    "preprocess_dataset(\"en_ewt-ud-test.conllu\", \"my_en_test.conllu\")\n",
    "\n",
    "# Preprocessing the french treebanks\n",
    "preprocess_dataset(\"fr_gsd-ud-train.conllu\", \"my_fr_train.conllu\")\n",
    "preprocess_dataset(\"fr_gsd-ud-dev.conllu\", \"my_fr_dev.conllu\")\n",
    "preprocess_dataset(\"fr_gsd-ud-test.conllu\", \"my_fr_test.conllu\")\n",
    "\n",
    "# Preprocessing the italian treebanks\n",
    "preprocess_dataset(\"it_isdt-ud-train.conllu\", \"my_it_train.conllu\")\n",
    "preprocess_dataset(\"it_isdt-ud-dev.conllu\", \"my_it_dev.conllu\")\n",
    "preprocess_dataset(\"it_isdt-ud-test.conllu\", \"my_it_test.conllu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to generate the \"proper\" datasets that will be used with our PoS Tagger. For this, we'll make use of the *generate_samples* function, defined in the preprocess.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English samples\n",
    "en_train_dataset = generate_samples(\"my_en_train.conllu\")\n",
    "en_val_dataset = generate_samples(\"my_en_dev.conllu\")\n",
    "en_test_dataset = generate_samples(\"my_en_test.conllu\")\n",
    "\n",
    "# French samples\n",
    "fr_train_dataset = generate_samples(\"my_fr_train.conllu\")\n",
    "fr_val_dataset = generate_samples(\"my_fr_dev.conllu\")\n",
    "fr_test_dataset = generate_samples(\"my_fr_test.conllu\")\n",
    "\n",
    "# Italian samples\n",
    "it_train_dataset = generate_samples(\"my_it_train.conllu\")\n",
    "it_val_dataset = generate_samples(\"my_it_dev.conllu\")\n",
    "it_test_dataset = generate_samples(\"my_it_test.conllu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our datasets prepared, we can proceed with the creation and training of the models that will perform the PoS Tagging. For that, we'll make use of the POS_Tagger class, defined in the tagger.py file, which allows us to easily perform all needed tasks in a concise manner.\n",
    "\n",
    "To begin, we only need to pass our datasets to the class. The order is (train, validation, test), although the class only really needs a training dataset. The rest can be omitted, although some functionality might not be available (for example, without a validation set we won't have validation metrics during training, and without a test set, we won't be able to test the final model unless we provide the dataset explicitly, and on a correct format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 20:03:40.237966: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-22 20:03:40.238478: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-11-22 20:03:40.238620: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-11-22 20:03:40.238719: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-11-22 20:03:40.238816: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-11-22 20:03:40.312150: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-11-22 20:03:40.312735: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-11-22 20:03:40.313820: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "english_tagger = POS_Tagger(en_train_dataset, en_val_dataset, en_test_dataset)\n",
    "\n",
    "french_tagger = POS_Tagger(fr_train_dataset, fr_val_dataset, fr_test_dataset)\n",
    "\n",
    "italian_tagger = POS_Tagger(it_train_dataset, it_val_dataset, it_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to actually build and compile the desired model. To do so, we use the build() method of the POS_Tagger class. On it, we may specify the number of PoS labels, and the output dimensions of the Embedding and LSTM layer. However, this is not strictly needed, as the default values work well enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tagger.build()\n",
    "\n",
    "french_tagger.build()\n",
    "\n",
    "italian_tagger.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to actually perform the training step. We can do so with the train() method of our tagger objects. It's possible to define the number of epochs of training (by default, it will train for 10 epochs, which can take a while depending on the computer's power)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "196/196 [==============================] - 15s 66ms/step - loss: 2.1113 - accuracy: 0.3965 - val_loss: 1.1848 - val_accuracy: 0.6870\n",
      "Epoch 2/10\n",
      "196/196 [==============================] - 13s 67ms/step - loss: 0.7223 - accuracy: 0.7949 - val_loss: 0.5861 - val_accuracy: 0.8398\n",
      "Epoch 3/10\n",
      "196/196 [==============================] - 14s 72ms/step - loss: 0.4021 - accuracy: 0.8905 - val_loss: 0.4402 - val_accuracy: 0.8806\n",
      "Epoch 4/10\n",
      "196/196 [==============================] - 14s 72ms/step - loss: 0.2725 - accuracy: 0.9259 - val_loss: 0.3904 - val_accuracy: 0.8904\n",
      "Epoch 5/10\n",
      "196/196 [==============================] - 14s 72ms/step - loss: 0.2091 - accuracy: 0.9405 - val_loss: 0.3727 - val_accuracy: 0.8925\n",
      "Epoch 6/10\n",
      "196/196 [==============================] - 14s 73ms/step - loss: 0.1735 - accuracy: 0.9478 - val_loss: 0.3696 - val_accuracy: 0.8929\n",
      "Epoch 7/10\n",
      "196/196 [==============================] - 14s 73ms/step - loss: 0.1503 - accuracy: 0.9535 - val_loss: 0.3721 - val_accuracy: 0.8931\n",
      "Epoch 8/10\n",
      "196/196 [==============================] - 14s 72ms/step - loss: 0.1331 - accuracy: 0.9579 - val_loss: 0.3802 - val_accuracy: 0.8921\n",
      "Epoch 9/10\n",
      "196/196 [==============================] - 14s 72ms/step - loss: 0.1194 - accuracy: 0.9617 - val_loss: 0.3871 - val_accuracy: 0.8914\n",
      "Epoch 10/10\n",
      "196/196 [==============================] - 14s 72ms/step - loss: 0.1085 - accuracy: 0.9648 - val_loss: 0.3994 - val_accuracy: 0.8912\n"
     ]
    }
   ],
   "source": [
    "english_tagger.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "226/226 [==============================] - 17s 67ms/step - loss: 1.6736 - accuracy: 0.5194 - val_loss: 0.7558 - val_accuracy: 0.7939\n",
      "Epoch 2/10\n",
      "226/226 [==============================] - 15s 65ms/step - loss: 0.4753 - accuracy: 0.8750 - val_loss: 0.3537 - val_accuracy: 0.9178\n",
      "Epoch 3/10\n",
      "226/226 [==============================] - 17s 73ms/step - loss: 0.2380 - accuracy: 0.9465 - val_loss: 0.2821 - val_accuracy: 0.9330\n",
      "Epoch 4/10\n",
      "226/226 [==============================] - 16s 73ms/step - loss: 0.1629 - accuracy: 0.9648 - val_loss: 0.2605 - val_accuracy: 0.9362\n",
      "Epoch 5/10\n",
      "226/226 [==============================] - 17s 73ms/step - loss: 0.1287 - accuracy: 0.9711 - val_loss: 0.2535 - val_accuracy: 0.9370\n",
      "Epoch 6/10\n",
      "226/226 [==============================] - 17s 74ms/step - loss: 0.1085 - accuracy: 0.9743 - val_loss: 0.2549 - val_accuracy: 0.9350\n",
      "Epoch 7/10\n",
      "226/226 [==============================] - 19s 84ms/step - loss: 0.0924 - accuracy: 0.9771 - val_loss: 0.2535 - val_accuracy: 0.9346\n",
      "Epoch 8/10\n",
      "226/226 [==============================] - 19s 83ms/step - loss: 0.0954 - accuracy: 0.9765 - val_loss: 0.2640 - val_accuracy: 0.9352\n",
      "Epoch 9/10\n",
      "226/226 [==============================] - 19s 83ms/step - loss: 0.0776 - accuracy: 0.9802 - val_loss: 0.2630 - val_accuracy: 0.9357\n",
      "Epoch 10/10\n",
      "226/226 [==============================] - 19s 83ms/step - loss: 0.0640 - accuracy: 0.9829 - val_loss: 0.2588 - val_accuracy: 0.9354\n"
     ]
    }
   ],
   "source": [
    "french_tagger.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "205/205 [==============================] - 20s 86ms/step - loss: 1.8442 - accuracy: 0.4722 - val_loss: 1.0693 - val_accuracy: 0.6847\n",
      "Epoch 2/10\n",
      "205/205 [==============================] - 17s 81ms/step - loss: 0.7182 - accuracy: 0.7941 - val_loss: 0.5263 - val_accuracy: 0.8520\n",
      "Epoch 3/10\n",
      "205/205 [==============================] - 17s 81ms/step - loss: 0.3665 - accuracy: 0.9008 - val_loss: 0.3608 - val_accuracy: 0.8975\n",
      "Epoch 4/10\n",
      "205/205 [==============================] - 17s 81ms/step - loss: 0.2329 - accuracy: 0.9404 - val_loss: 0.3046 - val_accuracy: 0.9117\n",
      "Epoch 5/10\n",
      "205/205 [==============================] - 17s 81ms/step - loss: 0.1745 - accuracy: 0.9543 - val_loss: 0.2766 - val_accuracy: 0.9212\n",
      "Epoch 6/10\n",
      "205/205 [==============================] - 17s 81ms/step - loss: 0.1418 - accuracy: 0.9619 - val_loss: 0.2646 - val_accuracy: 0.9247\n",
      "Epoch 7/10\n",
      "205/205 [==============================] - 17s 81ms/step - loss: 0.1207 - accuracy: 0.9671 - val_loss: 0.2578 - val_accuracy: 0.9269\n",
      "Epoch 8/10\n",
      "205/205 [==============================] - 17s 81ms/step - loss: 0.1056 - accuracy: 0.9705 - val_loss: 0.2551 - val_accuracy: 0.9280\n",
      "Epoch 9/10\n",
      "205/205 [==============================] - 17s 81ms/step - loss: 0.0940 - accuracy: 0.9730 - val_loss: 0.2546 - val_accuracy: 0.9298\n",
      "Epoch 10/10\n",
      "205/205 [==============================] - 17s 81ms/step - loss: 0.0846 - accuracy: 0.9753 - val_loss: 0.2577 - val_accuracy: 0.9306\n"
     ]
    }
   ],
   "source": [
    "italian_tagger.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the models and making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've trained our taggers, we can start using them to predict PoS Tags. Considering we have a test set, it might be useful to evaluate the performance of the models before \"freely\" tagging sentences with them. We can do so using the test() method of the tagger objects. It accepts a specific test set, but it will use the test set defined when creating the tagger if there was one, and none other is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 1s 26ms/step - loss: 0.3844 - accuracy: 0.8928\n",
      "test loss, test acc: [0.38438504934310913, 0.8928030729293823]\n"
     ]
    }
   ],
   "source": [
    "english_tagger.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "You must first build and train a model, by using build() and train(), before testing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfrench_tagger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Escritorio/NLU/nlu_1/tagger.py:92\u001b[0m, in \u001b[0;36mPOS_Tagger.test\u001b[0;34m(self, test_data)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_ds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m test_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must provide a test dataset, either to this call or when defining the Tagger\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# we check if the tagger's model exists and has been trained (that is, history is not empty)\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must first build and train a model, by using build() and train(), before testing\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# we prioritize user-defined datasets, and if they don't exist, we use the one stored in the object\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: You must first build and train a model, by using build() and train(), before testing"
     ]
    }
   ],
   "source": [
    "french_tagger.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_tagger.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this done, we can start tagging user-defined sentences. For this, we use the predict_sentence() method of the taggers. We should write our sentence as an argument, although there is a default sentence if we give none (however, keep in mind that sentence is in english, so it won't work well for other-language taggers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 555ms/step\n",
      "DET => this\n",
      "AUX => is\n",
      "DET => a\n",
      "NOUN => sample\n",
      "NOUN => sentence\n"
     ]
    }
   ],
   "source": [
    "english_tagger.predict_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "PRON => my\n",
      "NOUN => name\n",
      "AUX => is\n",
      "PROPN => John\n",
      "CCONJ => and\n",
      "PRON => I\n",
      "VERB => like\n",
      "NOUN => trees\n"
     ]
    }
   ],
   "source": [
    "english_tagger.predict_sentence(\"my name is John and I like trees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_tagger.predict_sentence(\"le chat mange un gros poisson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_tagger.predict_sentence(\"Pedro saltò da un albero e si ruppe la mano\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix 1: Saving and loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The POS_Tagger class implements the usual Tensorflow save/load functionality to alleviate the need for constant training (which can take time). When we have a satisfactory model, we might save it by calling the save() method. It will save the model on a folder, on the current working directory, and the folder can be named using the first argument of the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tagger.save(\"en_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_tagger.save(\"fr_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_tagger.save(\"it_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to load a model saved in this manner to a POS_Tagger object, we just need to call the load() method, passing the folder path as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_tagger_2 = POS_Tagger(it_train_dataset) #we still need to provide a training dataset, although we won't use it, because of how the class is implemented\n",
    "italian_tagger_2.load(\"it_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is loaded, we can skip the build/train/test part and directly use it. As we loaded the same model, the prediction for the sentence used before should be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_tagger_2.predict_sentence(\"Pedro saltò da un albero e si ruppe la mano\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
