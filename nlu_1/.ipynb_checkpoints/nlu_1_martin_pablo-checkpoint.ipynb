{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 20:02:19.643626: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-22 20:02:19.802144: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-22 20:02:19.822576: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-11-22 20:02:19.822590: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-11-22 20:02:20.460518: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-11-22 20:02:20.460558: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-11-22 20:02:20.460561: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from preprocess import *\n",
    "from tagger import POS_Tagger\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three datasets used during our tests are accessible on the following GitHub repositories:\n",
    "- English dataset: https://github.com/UniversalDependencies/UD_English-EWT\n",
    "- French dataset: https://github.com/UniversalDependencies/UD_French-GSD\n",
    "- Italian dataset: https://github.com/UniversalDependencies/UD_Italian-ISDT\n",
    "\n",
    "Below there is code to download the three sets of each dataset (train, dev/validation, test) to your computer. Feel free to comment/uncomment any lines to decide on what datasets you actually download (but keep in mind that you must do the same for the rest of the preprocessing steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(url, name):\n",
    "    filename = os.path.join(os.getcwd(), name) #we save on the working directory\n",
    "\n",
    "    r = requests.get(url) #get the webpage\n",
    "    with open(filename, 'w', encoding=\"utf-8\") as f: #and write it to a file\n",
    "      f.write(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English treebanks\n",
    "get_dataset(\"https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu\", \"en_ewt-ud-train.conllu\")\n",
    "get_dataset(\"https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-dev.conllu\", \"en_ewt-ud-dev.conllu\")\n",
    "get_dataset(\"https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-test.conllu\", \"en_ewt-ud-test.conllu\")\n",
    "\n",
    "# French treebanks\n",
    "get_dataset(\"https://raw.githubusercontent.com/UniversalDependencies/UD_French-GSD/master/fr_gsd-ud-train.conllu\", \"fr_gsd-ud-train.conllu\")\n",
    "get_dataset(\"https://raw.githubusercontent.com/UniversalDependencies/UD_French-GSD/master/fr_gsd-ud-dev.conllu\", \"fr_gsd-ud-dev.conllu\")\n",
    "get_dataset(\"https://raw.githubusercontent.com/UniversalDependencies/UD_French-GSD/master/fr_gsd-ud-test.conllu\", \"fr_gsd-ud-test.conllu\")\n",
    "\n",
    "# Italian treebanks\n",
    "get_dataset(\"https://raw.githubusercontent.com/UniversalDependencies/UD_Italian-ISDT/master/it_isdt-ud-train.conllu\", \"it_isdt-ud-train.conllu\")\n",
    "get_dataset(\"https://raw.githubusercontent.com/UniversalDependencies/UD_Italian-ISDT/master/it_isdt-ud-dev.conllu\", \"it_isdt-ud-dev.conllu\")\n",
    "get_dataset(\"https://raw.githubusercontent.com/UniversalDependencies/UD_Italian-ISDT/master/it_isdt-ud-test.conllu\", \"it_isdt-ud-test.conllu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocess the datasets downloaded, essentially stripping them from comments, multiwords, and empty tokens. This generates new .conllu files, that we'll use next to produce our data structures holding inputs and targets of the PoS Tagger. As said, feel free to comment/uncomment any lines referring to datasets you don't/do have.\n",
    "\n",
    "This makes use of the *preprocess_dataset* function, defined in the preprocess.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the english treebanks\n",
    "preprocess_dataset(\"en_ewt-ud-train.conllu\", \"my_en_train.conllu\")\n",
    "preprocess_dataset(\"en_ewt-ud-dev.conllu\", \"my_en_dev.conllu\")\n",
    "preprocess_dataset(\"en_ewt-ud-test.conllu\", \"my_en_test.conllu\")\n",
    "\n",
    "# Preprocessing the french treebanks\n",
    "preprocess_dataset(\"fr_gsd-ud-train.conllu\", \"my_fr_train.conllu\")\n",
    "preprocess_dataset(\"fr_gsd-ud-dev.conllu\", \"my_fr_dev.conllu\")\n",
    "preprocess_dataset(\"fr_gsd-ud-test.conllu\", \"my_fr_test.conllu\")\n",
    "\n",
    "# Preprocessing the italian treebanks\n",
    "preprocess_dataset(\"it_isdt-ud-train.conllu\", \"my_it_train.conllu\")\n",
    "preprocess_dataset(\"it_isdt-ud-dev.conllu\", \"my_it_dev.conllu\")\n",
    "preprocess_dataset(\"it_isdt-ud-test.conllu\", \"my_it_test.conllu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to generate the \"proper\" datasets that will be used with our PoS Tagger. For this, we'll make use of the *generate_samples* function, defined in the preprocess.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m en_test_dataset \u001b[38;5;241m=\u001b[39m generate_samples(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_en_test.conllu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# French samples\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m fr_train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy_fr_train.conllu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m fr_val_dataset \u001b[38;5;241m=\u001b[39m generate_samples(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_fr_dev.conllu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m fr_test_dataset \u001b[38;5;241m=\u001b[39m generate_samples(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_fr_test.conllu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Escritorio/NLU/nlu_1/preprocess.py:44\u001b[0m, in \u001b[0;36mgenerate_samples\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     41\u001b[0m   results\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(encode_UPOS, pos)) \u001b[38;5;66;03m# we execute the encoding function over all the tags stored in the pos array, and create a list out of the resulting labels\u001b[39;00m\n\u001b[1;32m     42\u001b[0m   \u001b[38;5;66;03m# we append a new row to our dataframe; the first column is a string, made by joining all the words of the sentence with spaces, and the second column\u001b[39;00m\n\u001b[1;32m     43\u001b[0m   \u001b[38;5;66;03m# is an array composed by all the labels we encoded before, and padded with zeros until it has exactly 128 elements.\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m   dataset\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mlen\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mindex)] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words), np\u001b[38;5;241m.\u001b[39marray(np\u001b[38;5;241m.\u001b[39mpad(results, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m128\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(results)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m, constant_values\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m)), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)]\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#we empty the arrays for words and tags, preparing them for the next sentence\u001b[39;00m\n\u001b[1;32m     46\u001b[0m words\u001b[38;5;241m=\u001b[39m[]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py:818\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[1;32m    817\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[0;32m--> 818\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py:1785\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1782\u001b[0m     indexer, missing \u001b[38;5;241m=\u001b[39m convert_missing_indexer(indexer)\n\u001b[1;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[0;32m-> 1785\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1789\u001b[0m     \u001b[38;5;66;03m# must come after setting of missing\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py:2182\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_missing\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m   2180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_mgr \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39m_mgr\n\u001b[1;32m   2181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_append(value)\u001b[38;5;241m.\u001b[39m_mgr\n\u001b[1;32m   2183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_maybe_update_cacher(clear\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py:5915\u001b[0m, in \u001b[0;36mNDFrame.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   5913\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   5914\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n\u001b[0;32m-> 5915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5916\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m   5917\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# English samples\n",
    "en_train_dataset = generate_samples(\"my_en_train.conllu\")\n",
    "en_val_dataset = generate_samples(\"my_en_dev.conllu\")\n",
    "en_test_dataset = generate_samples(\"my_en_test.conllu\")\n",
    "\n",
    "# French samples\n",
    "fr_train_dataset = generate_samples(\"my_fr_train.conllu\")\n",
    "fr_val_dataset = generate_samples(\"my_fr_dev.conllu\")\n",
    "fr_test_dataset = generate_samples(\"my_fr_test.conllu\")\n",
    "\n",
    "# Italian samples\n",
    "it_train_dataset = generate_samples(\"my_it_train.conllu\")\n",
    "it_val_dataset = generate_samples(\"my_it_dev.conllu\")\n",
    "it_test_dataset = generate_samples(\"my_it_test.conllu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our datasets prepared, we can proceed with the creation and training of the models that will perform the PoS Tagging. For that, we'll make use of the POS_Tagger class, defined in the tagger.py file, which allows us to easily perform all needed tasks in a concise manner.\n",
    "\n",
    "To begin, we only need to pass our datasets to the class. The order is (train, validation, test), although the class only really needs a training dataset. The rest can be omitted, although some functionality might not be available (for example, without a validation set we won't have validation metrics during training, and without a test set, we won't be able to test the final model unless we provide the dataset explicitly, and on a correct format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tagger = POS_Tagger(en_train_dataset, en_val_dataset, en_test_dataset)\n",
    "\n",
    "french_tagger = POS_Tagger(fr_train_dataset, fr_val_dataset, fr_test_dataset)\n",
    "\n",
    "italian_tagger = POS_Tagger(it_train_dataset, it_val_dataset, it_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to actually build and compile the desired model. To do so, we use the build() method of the POS_Tagger class. On it, we may specify the number of PoS labels, and the output dimensions of the Embedding and LSTM layer. However, this is not strictly needed, as the default values work well enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tagger.build()\n",
    "\n",
    "french_tagger.build()\n",
    "\n",
    "italian_tagger.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to actually perform the training step. We can do so with the train() method of our tagger objects. It's possible to define the number of epochs of training (by default, it will train for 10 epochs, which can take a while depending on the computer's power)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tagger.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#french_tagger.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#italian_tagger.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the models and making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've trained our taggers, we can start using them to predict PoS Tags. Considering we have a test set, it might be useful to evaluate the performance of the models before \"freely\" tagging sentences with them. We can do so using the test() method of the tagger objects. It accepts a specific test set, but it will use the test set defined when creating the tagger if there was one, and none other is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tagger.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_tagger.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_tagger.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this done, we can start tagging user-defined sentences. For this, we use the predict_sentence() method of the taggers. We should write our sentence as an argument, although there is a default sentence if we give none (however, keep in mind that sentence is in english, so it won't work well for other-language taggers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tagger.predict_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tagger.predict_sentence(\"my name is John and I like trees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_tagger.predict_sentence(\"le chat mange un gros poisson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_tagger.predict_sentence(\"Pedro saltò da un albero e si ruppe la mano\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix 1: Saving and loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The POS_Tagger class implements the usual Tensorflow save/load functionality to alleviate the need for constant training (which can take time). When we have a satisfactory model, we might save it by calling the save() method. It will save the model on a folder, on the current working directory, and the folder can be named using the first argument of the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tagger.save(\"en_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_tagger.save(\"fr_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_tagger.save(\"it_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to load a model saved in this manner to a POS_Tagger object, we just need to call the load() method, passing the folder path as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_tagger_2 = POS_Tagger(it_train_dataset) #we still need to provide a training dataset, although we won't use it, because of how the class is implemented\n",
    "italian_tagger_2.load(\"it_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is loaded, we can skip the build/train/test part and directly use it. As we loaded the same model, the prediction for the sentence used before should be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_tagger_2.predict_sentence(\"Pedro saltò da un albero e si ruppe la mano\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
